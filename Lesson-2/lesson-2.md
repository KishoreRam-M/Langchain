# ğŸ“Œ LESSON 2 â€” Getting Started with LangChain (Ollama-Only)

---

## 1ï¸âƒ£ What is this lesson about?

This lesson is about building your **first working LangChain pipeline** using:

* a **local LLM (Ollama)**
* a **prompt**
* a **chain (pipeline)**
* a **clean mental model**

No agents yet
No memory yet
No RAG yet

Just **core foundations**.

---

## 2ï¸âƒ£ Why does this lesson exist?

Most beginners fail because they:

* copy code
* donâ€™t know **who talks to whom**
* donâ€™t know **what runs where**
* donâ€™t know **what LangChain actually controls**

This lesson fixes that.

After this lesson, you will be able to explain:

> â€œLangChain does NOT run models. It orchestrates steps.â€

---

## 3ï¸âƒ£ Big-Picture Mental Model (Very Important)

### Think in layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Your Python Code         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangChain                â”‚  â† orchestration
â”‚ (prompts, chains)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChatOllama               â”‚  â† adapter
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama Server (local)    â”‚  â† runtime
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Model (llama3 etc.)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

LangChain **never touches the model directly**.

---

## 4ï¸âƒ£ Step 1 â€” Install Required Packages

### Why these packages?

* `langchain-core` â†’ base abstractions
* `langchain-ollama` â†’ Ollama adapter

### Install

```bash
pip install langchain langchain-ollama
```

If Ollama is not running:

```bash
ollama run llama3
```

Leave it running.

---

## 5ï¸âƒ£ Step 2 â€” Create the LLM Object

### Code

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3",
    temperature=0
)
```

---

### What is happening here?

#### â“ Is this calling the model?

âŒ No.

This only:

* stores configuration
* prepares an interface
* knows **where to send requests**

Actual inference happens **later**.

---

### Temperature (engineering meaning)

| Value | Effect                 |
| ----- | ---------------------- |
| 0.0   | deterministic, factual |
| 0.7   | balanced               |
| 1.0   | creative, risky        |

For:

* RAG â†’ `0`
* Agents â†’ `0â€“0.3`
* Creative writing â†’ `0.7+`

---

## 6ï¸âƒ£ Step 3 â€” Understand Messages (Chat Models)

LangChain uses **chat-style messages**, not raw strings.

### Message roles

* `system` â†’ rules, behavior
* `human` â†’ user input
* `ai` â†’ model output

This matches how modern LLMs work internally.

---

## 7ï¸âƒ£ Step 4 â€” Create a Prompt Template

### Why prompt templates?

Hard-coded strings break when:

* inputs change
* steps grow
* chains get reused

Templates give:

* structure
* safety
* reusability

---

### Code

```python
from langchain_core.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("human", "Explain {topic} in simple words.")
])
```

---

### What `{topic}` means

It is a **placeholder**, not a variable yet.

LangChain will:

1. validate inputs
2. inject values
3. format messages

This prevents prompt bugs.

---

## 8ï¸âƒ£ Step 5 â€” Build Your First Chain (LCEL)

This is the **core LangChain idea**.

### Code

```python
chain = prompt | llm
```

### Read this slowly:

```
prompt  â†’  llm
```

Meaning:

* take formatted prompt
* send it to the model
* get response

This `|` operator is called **LCEL (LangChain Expression Language)**.

---

## 9ï¸âƒ£ Step 6 â€” Invoke the Chain

### Code

```python
response = chain.invoke({
    "topic": "LangChain"
})

print(response.content)
```

---

### What happens internally (step-by-step)

1. `{topic}` = `"LangChain"`
2. Prompt is formatted
3. Messages are created
4. Sent to ChatOllama
5. HTTP request â†’ Ollama
6. Tokens generated
7. Tokens streamed back
8. Final message returned

---

## 10ï¸âƒ£ What Exactly Is `response`?

`response` is **NOT a string**.

It is an **AIMessage object**.

Thatâ€™s why we use:

```python
response.content
```

This design supports:

* metadata
* tool calls
* streaming
* future extensions

---

## 11ï¸âƒ£ System Design View

This is now a **real system**, not a script.

```
User Input
   â†“
Prompt Template
   â†“
Chain
   â†“
LLM (Ollama)
   â†“
Structured Response
```

You can:

* add another prompt
* add a parser
* add memory
* add tools

Without rewriting everything.

---

## 12ï¸âƒ£ Common Beginner Mistakes (Important)

### âŒ Mistake 1: Treating LangChain as magic

LangChain just moves data between components.

### âŒ Mistake 2: Thinking `ChatOllama` runs models

It only **calls** Ollama.

### âŒ Mistake 3: Ignoring message objects

Chat models are **not strings**.

---

## 13ï¸âƒ£ Production Considerations (Early but Critical)

Even at this stage:

* Each `.invoke()` = **one LLM call**
* Ollama = CPU-bound
* Long prompts = slow inference
* Chains hide complexity â†’ log everything later

---

## 14ï¸âƒ£ Interview Questions (Lesson-Level)

### Q1: What does LangChain control vs Ollama?

**Answer:**
LangChain controls orchestration; Ollama runs the model.

### Q2: What is LCEL?

**Answer:**
A declarative way to compose LLM pipelines using `|`.

### Q3: Why use prompt templates?

**Answer:**
They enforce structure, prevent injection bugs, and enable reuse.
