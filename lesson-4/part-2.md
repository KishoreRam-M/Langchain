# ðŸ“Œ LESSON 4 â€“ PART 2: FEW-SHOT PROMPTING

*(Theory + Practical + Real-time + Engineering + Interview + Mini Project)*

---

## ðŸ§  1ï¸âƒ£ THEORY â€” What is Few-Shot Prompting?

### Simple definition

**Few-shot prompting = teaching the model using examples inside the prompt.**

Instead of just saying *what to do*, you show:

* how inputs look
* how outputs should look

LLMs learn patterns extremely well from examples.

---

### Why it exists (real problem)

Instructions alone are often:

* misunderstood
* loosely followed
* ignored by small models (like local Ollama models)

Few-shot prompting **reduces ambiguity**.

---

### Mental model

Think of few-shot prompting like:

> â€œHere are 2 solved examples.
> Now solve the 3rd one the same way.â€

Exactly how humans learn.

---

## ðŸ› ï¸ 2ï¸âƒ£ PRACTICAL â€” Few-Shot Prompt in LangChain

### Base setup

```python
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate

llm = ChatOllama(model="llama3", temperature=0)
```

---

### Few-shot prompt template

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are an internal IT support assistant. Answer briefly."),
    
    ("human", "Question: How do I reset my company email password?"),
    ("ai", "Go to the internal portal â†’ Security â†’ Reset Password."),
    
    ("human", "Question: How do I request laptop access?"),
    ("ai", "Submit a ticket in the IT portal under Hardware Requests."),
    
    ("human", "Question: {question}")
])

chain = prompt | llm
```

---

### Run it

```python
response = chain.invoke({
    "question": "How do I get VPN access?"
})

print(response.content)
```

Youâ€™ll see:

* consistent style
* short answers
* less hallucination

---

## ðŸŒ 3ï¸âƒ£ REAL-TIME SCENARIO

### ðŸŽ¯ Scenario: Company Internal Helpdesk Bot

Employees ask:

* IT questions
* access issues
* process-related doubts

Constraints:

* No long explanations
* No guessing
* Consistent format

Few-shot prompting ensures:

* uniform answers
* policy-safe responses
* predictable behavior

---

## âš™ï¸ 4ï¸âƒ£ ENGINEERING POV â€” When to Use / Avoid Few-Shot

### âœ… Use few-shot when:

* Output format matters
* Model keeps hallucinating
* You need consistent tone
* Using smaller local models

### âŒ Avoid few-shot when:

* Prompt becomes very large
* Latency is critical
* Examples change frequently (dynamic data)

ðŸ‘‰ In production, **few-shot = quality vs latency tradeoff**.

---

## ðŸž 5ï¸âƒ£ FAILURE MODE (Real Engineering Bug)

### Problem

Too many examples â†’ slow responses.

### Fix

* Use **2â€“3 high-quality examples**
* Keep examples short
* Move repeated logic to system message

---

## ðŸŽ¯ 6ï¸âƒ£ INTERVIEW QUESTIONS (With Answers)

### Q1ï¸âƒ£ What is few-shot prompting?

**Answer:**
A technique where example input-output pairs are included in the prompt to guide model behavior.

---

### Q2ï¸âƒ£ Why does few-shot work better than instructions alone?

**Answer:**
Because LLMs are pattern learners and follow demonstrated behavior more reliably than abstract instructions.

---

### Q3ï¸âƒ£ What is the main downside of few-shot prompting?

**Answer:**
Increased prompt length, which affects latency and cost.

---

## ðŸ§© 7ï¸âƒ£ MINI REAL-TIME PROJECT (IMPORTANT)

### ðŸ› ï¸ Project: **Local FAQ Assistant (Ollama + LangChain)**

This project uses **ALL skills so far**:

* ChatOllama
* Prompt templates
* System messages
* Few-shot prompting
* Debugging mindset

---

### ðŸ“‹ Requirements

Build a CLI or function that:

* Answers FAQs
* Uses few-shot examples
* Says **â€œI donâ€™t knowâ€** if unsure
* Answers in **2 lines max**

---

### ðŸ§  Prompt Design

```python
prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are a company FAQ assistant. "
     "Answer in max 2 lines. "
     "If you don't know, say 'I don't know'."),
    
    ("human", "Question: What is company leave policy?"),
    ("ai", "You get 20 paid leaves per year as per HR policy."),
    
    ("human", "Question: How to apply for work from home?"),
    ("ai", "Submit a request via the HR portal."),
    
    ("human", "Question: {question}")
])
```

---

### ðŸ§ª Run

```python
chain = prompt | llm

while True:
    q = input("Ask a question: ")
    if q.lower() == "exit":
        break
    print(chain.invoke({"question": q}).content)
```

---

## ðŸ§  What This Mini Project Teaches You

* How real bots are built
* Why prompt structure matters
* How to control hallucinations
* How Ollama behaves in production-like usage
* How LangChain glues everything safely

---

## ðŸ”š FINAL SUMMARY (Lesson 4 Complete)

* Few-shot prompting improves reliability
* Examples > instructions
* Tradeoff = latency vs accuracy
* Prompt = engineering contract
* Small models demand better prompts
